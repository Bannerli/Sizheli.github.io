---
title: 新时代的数据资产
date: 2025-02-19 11:10:17
tags:
---
## Big Data 遇到了什么瓶颈？
一提及Big Data，人们总是会提到数据指数级增长。然而，这个老生常谈的观点其实只关注了Big Data背后的 Volume 和 Velocity 特征，而对于适应此纬度的过度关注却容易让人忽视了更重要的 Value。
数据真的都大到1以PB级以上的数据集存在吗？从数据中挖掘出价值的瓶颈真的在于数据处理性能吗？
```
Most of the people using "Big Query" don't really have Big Data. Most people do not have that much data. I can say that the vast majority of customers had less than 1TB of data in total data storage. The general feedback we got to talking to folks in the industry was that 100 GB was the right order of magnitude for a data warehouse.
```
数据的分布遵从 Zipf Law, 即：若以数据量大小为纵轴、以该大小的数据量或者具有该数据量的个人/组织数量为横轴，会发现整体分布呈现幂律分布特征。

现实：没有一个系统能足够优秀到能以99分的成绩容纳所有的数据分析Workload。当某个Workload重要到其需求无法被一个中规中矩的平台满足时，使用者往往会选择采用一个性价比更高或性能更卷的技术栈。

目前的大部分Data Stack，在解决“数据孤岛”问题的逻辑上是相同的：我自己建我内部的计算引擎，存储引擎，连接池等各个组件，等到需要分析数据时，我先把各个源头的数据搬运到我自己内部，用我更擅长的数据结构重构后存储，这样我就能用自己的引擎更高更快更强地进行混合查询，让数据联合起来释放更多的价值。

然而，技术供应商往往挤破头卷性能，仅为头部量级的数据处理提供解决方案，却对长尾的不同量级的数据集的分析处理优化视而不见，这导致能将数据从价值有限的困境中解放出来的场景无人问津。